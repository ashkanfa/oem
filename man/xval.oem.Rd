% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/oem_xval.R
\name{xval.oem}
\alias{xval.oem}
\title{Orthogonalizing EM}
\usage{
xval.oem(x, y, nfolds = 10L, foldid = NULL, type.measure = c("mse",
  "deviance", "class", "auc", "mae"), ncores = -1, family = c("gaussian",
  "binomial"), penalty = c("elastic.net", "lasso", "ols", "mcp", "scad",
  "grp.lasso"), weights = numeric(0), lambda = numeric(0), nlambda = 100L,
  lambda.min.ratio = NULL, alpha = 1, gamma = 3, groups = numeric(0),
  penalty.factor = NULL, group.weights = NULL, standardize = TRUE,
  intercept = TRUE, maxit = 500L, tol = 1e-07, irls.maxit = 100L,
  irls.tol = 0.001, compute.loss = FALSE)
}
\arguments{
\item{x}{input matrix or SparseMatrix (sparse not yet implemented. 
Each row is an observation, each column corresponds to a covariate}

\item{y}{numeric response vector of length nobs.}

\item{nfolds}{integer number of cross validation folds. 3 is the minimum number allowed. defaults to 10}

\item{foldid}{an optional vector of values between 1 and nfold specifying which fold each observation belongs to.}

\item{type.measure}{measure to evaluate for cross-validation. The default is type.measure="deviance", 
which uses squared-error for gaussian models (a.k.a type.measure="mse" there), deviance for logistic
regression. type.measure="class" applies to binomial only. type.measure="auc" is for two-class logistic 
regression only. type.measure="mse" or type.measure="mae" (mean absolute error) can be used by all models;
they measure the deviation from the fitted mean to the response.}

\item{ncores}{Integer scalar that specifies the number of threads to be used}

\item{family}{"gaussian" for least squares problems, "binomial" for binary response.}

\item{penalty}{Specification of penalty type in lowercase letters. Choices include "lasso", 
"ols" (Ordinary least squares, no penaly), "elastic.net", "scad", "mcp", "grp.lasso"}

\item{weights}{observation weights. defaults to 1 for each observation (setting weight vector to 
length 0 will default all weights to 1)}

\item{lambda}{A user supplied lambda sequence. By default, the program computes
its own lambda sequence based on nlambda and lambda.min.ratio. Supplying
a value of lambda overrides this.}

\item{nlambda}{The number of lambda values - default is 100.}

\item{lambda.min.ratio}{Smallest value for lambda, as a fraction of lambda.max, the (data derived) entry
value (i.e. the smallest value for which all coefficients are zero). The default
depends on the sample size nobs relative to the number of variables nvars. If
nobs > nvars, the default is 0.0001, close to zero. If nobs < nvars, the default
is 0.01. A very small value of lambda.min.ratio will lead to a saturated fit
when nobs < nvars.}

\item{alpha}{mixing value for elastic.net. penalty applied is (1 - alpha) * (ridge penalty) + alpha * (lasso penalty)}

\item{gamma}{tuning parameter for SCAD and MCP penalties. must be >= 1}

\item{groups}{A vector of describing the grouping of the coefficients. See the example below. All unpenalized variables
should be put in group 0}

\item{penalty.factor}{Separate penalty factors can be applied to each coefficient. 
This is a number that multiplies lambda to allow differential shrinkage. Can be 0 for some variables, 
which implies no shrinkage, and that variable is always included in the model. Default is 1 for all 
variables.}

\item{group.weights}{penalty factors applied to each group for the group lasso. Similar to penalty.factor, 
this is a number that multiplies lambda to allow differential shrinkage. Can be 0 for some groups, 
which implies no shrinkage, and that group is always included in the model. Default is sqrt(group size) for all
groups.}

\item{standardize}{Logical flag for x variable standardization, prior to fitting the models. 
The coefficients are always returned on the original scale. Default is standardize=FALSE. If 
variables are in the same units already, you might not wish to standardize.}

\item{intercept}{Should intercept(s) be fitted (default=TRUE) or set to zero (FALSE)}

\item{maxit}{integer. Maximum number of OEM iterations}

\item{tol}{convergence tolerance for OEM iterations}

\item{irls.maxit}{integer. Maximum number of IRLS iterations}

\item{irls.tol}{convergence tolerance for IRLS iterations. Only used if family != "gaussian"}

\item{compute.loss}{should the loss be computed for each estimated tuning parameter? Defaults to FALSE. Setting
to TRUE will dramatically increase computational time}
}
\value{
An object with S3 class "xval.oem"
}
\description{
Orthogonalizing EM
}
\examples{
set.seed(123)
n.obs <- 1e4
n.vars <- 100

true.beta <- c(runif(15, -0.25, 0.25), rep(0, n.vars - 15))

x <- matrix(rnorm(n.obs * n.vars), n.obs, n.vars)
y <- rnorm(n.obs, sd = 3) + x \%*\% true.beta

system.time(fit <- oem(x = x, y = y, 
                       penalty = c("lasso", "grp.lasso"), 
                       groups = rep(1:20, each = 5)))
                       
system.time(xfit <- xval.oem(x = x, y = y, 
                             penalty = c("lasso", "grp.lasso"), 
                             groups = rep(1:20, each = 5)))

layout(matrix(1:2, ncol = 2))
plot(fit)
plot(fit, which.model = 2)

# logistic
y <- rbinom(n.obs, 1, prob = 1 / (1 + exp(-x \%*\% true.beta)))

system.time(res <- oem(x, y, intercept = FALSE, 
                       penalty = "lasso", 
                       family = "binomial", 
                       irls.tol = 1e-3, tol = 1e-8))

system.time(res.gr <- oem(x, y, intercept = FALSE, 
                          penalty = "grp.lasso", 
                          family = "binomial", 
                          groups = rep(1:10, each = 10), 
                          irls.tol = 1e-3, tol = 1e-8))

layout(matrix(1:2, ncol = 2))
plot(res)
plot(res.gr)

}

